{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webpage word count\n",
    "### With example of world country pages on Wikipedia\n",
    "An application that extracts the content of a website and displays the most occurring words on that webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/maciejtarsa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/maciejtarsa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request, urllib.error\n",
    "import operator\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set word limit\n",
    "# ie how many times a word has to occur for it to be displayed\n",
    "# for countries on wikipedia it is set to 20\n",
    "word_limit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to vectorize the tokens\n",
    "# ie count the occurance of each token in the text\n",
    "# takes a list of tokens as input\n",
    "# returns a dictionary containing features as keys and weights as values\n",
    "def vectorize(tokens):\n",
    "    # an empty dictionary to be returned at the end\n",
    "    features = {}\n",
    "    # iterate through all tokens\n",
    "    for token in tokens:\n",
    "        # check if that token already exists in the dictionary\n",
    "        try:\n",
    "            i = features[token]\n",
    "            # if it does, increment the count\n",
    "            features[token] = i + 1\n",
    "        # otherwise, assign a count of 1\n",
    "        except KeyError:\n",
    "            features[token] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function to check that the URL is reachable\n",
    "# implementation from:\n",
    "# https://stackoverflow.com/questions/1726402/in-python-how-do-i-use-urllib-to-see-if-a-website-is-404-or-200\n",
    "def check_URL(url):\n",
    "    try:\n",
    "        conn = urllib.request.urlopen(url)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        # Return code error (e.g. 404, 501, ...)\n",
    "        # ...\n",
    "        print('HTTPError: {}'.format(e.code))\n",
    "    except urllib.error.URLError as e:\n",
    "        # Not an HTTP-specific error (e.g. connection refused)\n",
    "        # ...\n",
    "        print('URLError: {}'.format(e.reason))\n",
    "    else:\n",
    "        # 200\n",
    "        # ...\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that counts the occurance of words on a webpage\n",
    "# takes a url as input\n",
    "# returns a dictionary of words and their occurance frequency \n",
    "def top_words(url):\n",
    "    \n",
    "    #check the URL\n",
    "    response = check_URL(url)\n",
    "    # if rerponse is True, continue\n",
    "    if response:\n",
    "        \n",
    "        # specify the url of the web page\n",
    "        source = urlopen(url).read()\n",
    "  \n",
    "        # make a soup \n",
    "        soup = BeautifulSoup(source,'lxml')\n",
    "        # extract the plain text content from paragraphs\n",
    "        text = ''\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            text += paragraph.text\n",
    "\n",
    "        # tokenize the words\n",
    "        tokens = word_tokenize(text, language=\"english\")\n",
    "        # remove stopwords\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "        # remove punctuation\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # vectorize tokens - count occurance of each word\n",
    "        vectors = vectorize(tokens)\n",
    "        # only keep the ones with counts over 20\n",
    "        vectors_reduced = {key:value for (key,value) in vectors.items() if value >= word_limit}\n",
    "        # sort the result in descending order\n",
    "        vectors_sorted =dict(sorted(vectors_reduced.items(), key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "        return vectors_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - country pages on wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1\n",
    "User specifies the country (or other wikipedia page name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input country: United Kingdomm\n",
      "HTTPError: 404\n"
     ]
    }
   ],
   "source": [
    "# ask the user for input\n",
    "user_input = input('Please input country: ')\n",
    "# convert spaces to underscores\n",
    "user_input_cleaned = user_input.replace(' ', '_')\n",
    "# set up a full path for URL\n",
    "url = 'https://en.wikipedia.org/wiki/' + user_input_cleaned\n",
    "# run the function to extract the top words\n",
    "top_words(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2\n",
    "Hardcode the full URL - copy and paste from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UK': 177,\n",
       " 'per': 128,\n",
       " 'British': 114,\n",
       " 'cent': 114,\n",
       " 'United': 113,\n",
       " 'Kingdom': 104,\n",
       " 'Ireland': 101,\n",
       " 'England': 91,\n",
       " 'Britain': 77,\n",
       " 'Wales': 70,\n",
       " 'Scotland': 68,\n",
       " 'Northern': 64,\n",
       " 'world': 64,\n",
       " 'population': 49,\n",
       " 'Great': 39,\n",
       " 'million': 34,\n",
       " 'first': 34,\n",
       " 'century': 34,\n",
       " 'London': 32,\n",
       " 'people': 29,\n",
       " 'Scottish': 27,\n",
       " 'Welsh': 27,\n",
       " 'English': 26,\n",
       " 'also': 26,\n",
       " 'government': 26,\n",
       " 'around': 26,\n",
       " 'Europe': 25,\n",
       " 'include': 25,\n",
       " 'country': 24,\n",
       " 'countries': 24,\n",
       " 'Union': 24,\n",
       " 'number': 24,\n",
       " 'largest': 23,\n",
       " 'including': 22,\n",
       " 'European': 21,\n",
       " 'Irish': 20,\n",
       " 'international': 20,\n",
       " 'one': 20,\n",
       " 'national': 20}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up a full path for URL\n",
    "url = 'https://en.wikipedia.org/wiki/United_Kingdom'\n",
    "# run the function to extract top words\n",
    "top_words(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
